{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "## Libraries\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "## Custom Libararies\n",
    "from src.logger_v1 import setup_logs\n",
    "from src.data_reader.dataset import RawDataset, ReverseRawDataset, RawXXreverseDataset\n",
    "from src.training_v1 import train, trainXXreverse, snapshot\n",
    "from src.validation_v1 import validation, validationXXreverse\n",
    "from src.model.model import CDCK2, CDCK5, CDCK6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"cdc\" + time.strftime(\"-%Y-%m-%d_%H_%M_%S\")\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer wrapper class for lr scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim(object):\n",
    "    \"\"\"A simple wrapper class for learning rate scheduling\"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, n_warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = 128 \n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0 \n",
    "        self.delta = 1\n",
    "\n",
    "    def state_dict(self):\n",
    "        self.optimizer.state_dict()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Step by the inner optimizer\"\"\"\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out the gradients by the inner optimizer\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def increase_delta(self):\n",
    "        self.delta *= 2\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        \"\"\"Learning rate scheduling per step\"\"\"\n",
    "\n",
    "        self.n_current_steps += self.delta\n",
    "        new_lr = np.power(self.d_model, -0.5) * np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "        return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argument parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--train-raw', required=True)\n",
    "    parser.add_argument('--validation-raw', required=True)\n",
    "    parser.add_argument('--eval-raw')\n",
    "    parser.add_argument('--train-list', required=True)\n",
    "    parser.add_argument('--validation-list', required=True)\n",
    "    parser.add_argument('--eval-list')\n",
    "    parser.add_argument('--logging-dir', required=True,\n",
    "                        help='model save directory')\n",
    "    parser.add_argument('--epochs', type=int, default=60, metavar='N',\n",
    "                        help='number of epochs to train')\n",
    "    parser.add_argument('--n-warmup-steps', type=int, default=50)\n",
    "    parser.add_argument('--batch-size', type=int, default=64, \n",
    "                        help='batch size')\n",
    "    parser.add_argument('--audio-window', type=int, default=20480, \n",
    "                        help='window length to sample from each utterance')\n",
    "    parser.add_argument('--timestep', type=int, default=12) \n",
    "    parser.add_argument('--masked-frames', type=int, default=20)\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[\n",
    "        '--train-raw', 'LibriSpeech/train-Librispeech.h5' ,\n",
    "        '--validation-raw', 'LibriSpeech/dev-Librispeech.h5' ,\n",
    "        '--eval-raw', 'LibriSpeech/test-Librispeech.h5' ,\n",
    "        '--train-list', 'LibriSpeech/list/train.txt' ,\n",
    "        '--validation-list', 'LibriSpeech/list/dev.txt',\n",
    "        '--eval-list', 'LibriSpeech/list/test.txt' ,\n",
    "        '--logging-dir', 'snapshot/cdc/' ,\n",
    "        '--log-interval', '50' ,\n",
    "        '--audio-window', '20480' ,\n",
    "        '--timestep', '12' ,\n",
    "        '--masked-frames', '10' ,\n",
    "        '--n-warmup-steps', '1000'\n",
    "])\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print('use_cuda is', use_cuda)\n",
    "global_timer = timer() # global timer\n",
    "logger = setup_logs(args.logging_dir, run_name) # setup logs\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "## Loading the dataset\n",
    "params = {'num_workers': 0,\n",
    "          'pin_memory': False} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CDCK2(args.timestep, args.batch_size, args.audio_window).to(device)\n",
    "#model = CDCK5(args.timestep, args.batch_size, args.audio_window).to(device)\n",
    "#model = CDCK6(args.timestep, args.batch_size, args.audio_window).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting logger, dataset, dataloader, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('===> loading train, validation and eval dataset')\n",
    "training_set   = RawDataset(args.train_raw, args.train_list, args.audio_window)\n",
    "#training_set   = ReverseRawDataset(args.train_raw, args.train_list, args.audio_window)\n",
    "#training_set   = RawXXreverseDataset(args.train_raw, args.train_list, args.audio_window)\n",
    "validation_set = RawDataset(args.validation_raw, args.validation_list, args.audio_window)\n",
    "#validation_set = ReverseRawDataset(args.validation_raw, args.validation_list, args.audio_window)\n",
    "#validation_set = RawXXreverseDataset(args.validation_raw, args.validation_list, args.audio_window)\n",
    "train_loader = data.DataLoader(training_set, batch_size=args.batch_size, shuffle=True, **params) # set shuffle to True\n",
    "validation_loader = data.DataLoader(validation_set, batch_size=args.batch_size, shuffle=False, **params) # set shuffle to False\n",
    "# nanxin optimizer  \n",
    "optimizer = ScheduledOptim(\n",
    "    optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        betas=(0.9, 0.98), eps=1e-09, weight_decay=1e-4, amsgrad=True),\n",
    "    args.n_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info('### Model summary below###\\n {}\\n'.format(str(model)))\n",
    "logger.info('===> Model total parameter: {}\\n'.format(model_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start training\n",
    "best_acc = 0\n",
    "best_loss = np.inf\n",
    "best_epoch = -1 \n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    epoch_timer = timer()\n",
    "\n",
    "    # Train and validate\n",
    "    #trainXXreverse(args, model, device, train_loader, optimizer, epoch, args.batch_size)\n",
    "    #val_acc, val_loss = validationXXreverse(args, model, device, validation_loader, args.batch_size)\n",
    "    train(args, model, device, train_loader, optimizer, epoch, args.batch_size)\n",
    "    val_acc, val_loss = validation(args, model, device, validation_loader, args.batch_size)\n",
    "\n",
    "    # Save\n",
    "    if val_acc > best_acc: \n",
    "        best_acc = max(val_acc, best_acc)\n",
    "        snapshot(args.logging_dir, run_name, {\n",
    "            'epoch': epoch + 1,\n",
    "            'validation_acc': val_acc, \n",
    "            'state_dict': model.state_dict(),\n",
    "            'validation_loss': val_loss,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        })\n",
    "        best_epoch = epoch + 1\n",
    "    elif epoch - best_epoch > 2:\n",
    "        optimizer.increase_delta()\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "    end_epoch_timer = timer()\n",
    "    logger.info(\"#### End epoch {}/{}, elapsed time: {}\".format(epoch, args.epochs, end_epoch_timer - epoch_timer))\n",
    "\n",
    "## end \n",
    "end_global_timer = timer()\n",
    "logger.info(\"################## Success #########################\")\n",
    "logger.info(\"Total elapsed time: %s\" % (end_global_timer - global_timer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
